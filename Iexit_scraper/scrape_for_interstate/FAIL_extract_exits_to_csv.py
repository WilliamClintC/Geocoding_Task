from bs4 import BeautifulSoup
import re
import csv
import os
import requests
import time
import json
import subprocess
from urllib.parse import urljoin

def extract_exit_info_with_js_scraper(url, output_csv_path):
    """
    Use the JavaScript scraper to get exit information directly from the specific URL
    
    Args:
        url (str): iExit URL to scrape
        output_csv_path (str): Path for the output CSV file
    """
    
    print("üöÄ Using JavaScript scraper for web-based extraction...")
    
    # Path to the JavaScript scraper
    js_scraper_path = os.path.join(os.path.dirname(__file__), 'scrape_for_interstate', 'iexit_scraper.js')
    
    if not os.path.exists(js_scraper_path):
        raise FileNotFoundError(f"JavaScript scraper not found at: {js_scraper_path}")
    
    try:
        # Run the JavaScript scraper with the specific URL
        print("üìã Note: The JavaScript scraper will open a browser window.")
        print("üîß You may need to manually solve CAPTCHAs if prompted.")
        
        # Change to the scraper directory
        scraper_dir = os.path.dirname(js_scraper_path)
        
        # Run the JavaScript scraper with the target URL as argument
        result = subprocess.run(
            ['node', 'iexit_scraper.js', '--target-url', url],
            cwd=scraper_dir,
            capture_output=False,  # Allow interactive input
            text=True,
            timeout=300  # 5 minutes timeout
        )
        
        if result.returncode != 0:
            print(f"‚ùå JavaScript scraper failed with return code: {result.returncode}")
            raise Exception(f"JavaScript scraper failed with return code: {result.returncode}")
        
        print("‚úÖ JavaScript scraper completed successfully")
        
        # Look for the generated exit details CSV file
        csv_files = [f for f in os.listdir(scraper_dir) if f.startswith('iexit_exit_details_') and f.endswith('.csv')]
        
        if not csv_files:
            print("‚ö†Ô∏è  No exit details CSV found, looking for states CSV as fallback...")
            # Fallback to processing states CSV
            csv_files = [f for f in os.listdir(scraper_dir) if f.startswith('iexit_states_exits_') and f.endswith('.csv')]
            
            if not csv_files:
                raise Exception("No CSV file generated by JavaScript scraper")
            
            # Use the most recent states file and process it
            latest_csv = max(csv_files, key=lambda f: os.path.getctime(os.path.join(scraper_dir, f)))
            latest_csv_path = os.path.join(scraper_dir, latest_csv)
            
            print(f"üìÑ Found generated states CSV: {latest_csv_path}")
            return process_scraped_data_for_url(latest_csv_path, url, output_csv_path)
        
        # Use the most recent exit details file
        latest_csv = max(csv_files, key=lambda f: os.path.getctime(os.path.join(scraper_dir, f)))
        latest_csv_path = os.path.join(scraper_dir, latest_csv)
        
        print(f"üìÑ Found generated exit details CSV: {latest_csv_path}")
        
        # Copy the exit details file to the desired output path
        import shutil
        shutil.copy2(latest_csv_path, output_csv_path)
        
        # Read and return the exit data
        with open(output_csv_path, 'r', encoding='utf-8') as file:
            reader = csv.DictReader(file)
            exits = list(reader)
        
        return exits
        
    except subprocess.TimeoutExpired:
        raise Exception("JavaScript scraper timed out after 5 minutes")
    except Exception as e:
        raise Exception(f"JavaScript scraper execution failed: {e}")

def process_scraped_data_for_url(scraped_csv_path, target_url, output_csv_path):
    """
    Process the scraped data to extract exits for a specific URL
    
    Args:
        scraped_csv_path (str): Path to the CSV generated by JavaScript scraper
        target_url (str): The specific URL we want exits for
        output_csv_path (str): Path for the output CSV file
    """
    
    print(f"üìä Processing scraped data for URL: {target_url}")
    
    # Read the scraped CSV
    with open(scraped_csv_path, 'r', encoding='utf-8') as file:
        reader = csv.DictReader(file)
        scraped_data = list(reader)
    
    # Find the matching exit link
    matching_exits = []
    for row in scraped_data:
        if row['Exit_Link'] == target_url:
            matching_exits.append(row)
    
    if not matching_exits:
        print("‚ö†Ô∏è  No matching exits found in scraped data")
        print(f"   Looking for: {target_url}")
        print("   Available URLs:")
        for row in scraped_data[:5]:  # Show first 5 for reference
            print(f"     - {row['Exit_Link']}")
        
        # Try to fetch the URL directly
        print("üîÑ Attempting direct URL fetch...")
        return extract_exit_info_from_direct_url(target_url, output_csv_path)
    
    print(f"‚úÖ Found {len(matching_exits)} matching entries")
    
    # For now, we'll need to fetch the actual exit page
    # This is a placeholder - we'll implement the actual exit page scraping
    return extract_exit_info_from_direct_url(target_url, output_csv_path)

def extract_exit_info_from_direct_url(url, output_csv_path):
    """
    Extract exit information directly from iExit URL and save to CSV
    
    Args:
        url (str): iExit URL to scrape
        output_csv_path (str): Path for the output CSV file
    """
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/137.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.9',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1'
    }
    
    try:
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        html_content = response.text
        soup = BeautifulSoup(html_content, 'html.parser')
        
        print("‚úÖ Successfully fetched HTML content from URL")
        
        # Continue with the existing extraction logic
        return extract_exit_info_from_soup(soup, html_content, output_csv_path)
        
    except requests.RequestException as e:
        print(f"‚ùå Failed to fetch HTML content from URL: {e}")
        raise

def extract_exit_info_from_soup(soup, html_content, output_csv_path):
    """
    Extract exit information from BeautifulSoup object and save to CSV
    
    Args:
        soup: BeautifulSoup object of the HTML
        html_content (str): Raw HTML content
        output_csv_path (str): Path for the output CSV file
    """
    
    # Extract direction from the HTML
    print("Extracting direction information...")
    direction = extract_direction_from_html(soup, "web_url")
    print(f"Direction: {direction}")
    
    # Extract coordinates from JavaScript
    print("Extracting coordinates from JavaScript...")
    coordinates_data = extract_coordinates_from_javascript(html_content)
    print(f"Found coordinates for {len(coordinates_data)} exits")
    
    exits = []
    
    # Find all exit rows
    exit_rows = soup.find_all('tr', class_='list_exit_row_container_tr')
    
    print(f"Found {len(exit_rows)} exit rows")
    
    for row in exit_rows:
        exit_info = {}
        
        # Find exit sign (exit number/name)
        exit_sign_lines = row.find_all('div', class_='exitsignline')
        if exit_sign_lines:
            # Combine all exit sign lines (usually "EXIT" and the number)
            exit_name_parts = [line.get_text().strip() for line in exit_sign_lines if line.get_text().strip()]
            exit_info['exit_name'] = ' '.join(exit_name_parts)
        
        # Find exit description (this is usually the clickable link)
        exit_desc = row.find('div', class_='exitdescription')
        if exit_desc:
            # Get the text content, whether it's in a link or not
            exit_info['exit_description'] = exit_desc.get_text().strip()
        
        # Find exit location
        exit_location = row.find('div', class_='exitlocation')
        if exit_location:
            exit_info['exit_location'] = exit_location.get_text().strip()
        
        # Find iExit detail page link (the entire exit row is a link)
        iexit_link = row.find('a', class_='list_exit_row_container')
        if iexit_link:
            exit_info['iexit_detail_link'] = iexit_link.get('href')
            # Make it a full URL if it's relative
            if exit_info['iexit_detail_link'].startswith('/'):
                exit_info['iexit_detail_link'] = 'https://www.iexitapp.com' + exit_info['iexit_detail_link']
        else:
            exit_info['iexit_detail_link'] = 'N/A'
        
        # Initialize coordinate fields
        exit_info['latitude'] = 'N/A'
        exit_info['longitude'] = 'N/A'
        exit_info['google_maps_link'] = 'N/A'
        exit_info['direction'] = direction  # Add direction to each exit record
        
        # Try to find coordinates for this exit using the exit name
        if exit_info.get('exit_name'):
            # Try exact match first
            if exit_info['exit_name'] in coordinates_data:
                coord_data = coordinates_data[exit_info['exit_name']]
                exit_info['latitude'] = coord_data['latitude']
                exit_info['longitude'] = coord_data['longitude']
                exit_info['google_maps_link'] = coord_data['google_maps_link']
            else:
                # Try partial matches (in case format is slightly different)
                exit_name_clean = exit_info['exit_name'].upper().strip()
                exit_desc_clean = exit_info.get('exit_description', '').upper().strip()
                found_match = False
                
                for coord_title, coord_data in coordinates_data.items():
                    coord_title_clean = coord_title.upper().strip()
                    
                    # Check various matching patterns
                    if (exit_name_clean in coord_title_clean or 
                        coord_title_clean in exit_name_clean or
                        # Try matching against exit description for non-exit places
                        exit_desc_clean in coord_title_clean or
                        coord_title_clean in exit_desc_clean or
                        # Handle "TURN OUT" vs "Truck Turnout"
                        ('TURN' in exit_name_clean and 'TURNOUT' in coord_title_clean.replace(' ', '')) or
                        ('TURNOUT' in exit_name_clean.replace(' ', '') and 'TURN' in coord_title_clean) or
                        # Handle "WELCOME CENTER" variations
                        ('WELCOME' in exit_name_clean and 'WELCOME' in coord_title_clean)):
                        
                        exit_info['latitude'] = coord_data['latitude']
                        exit_info['longitude'] = coord_data['longitude']
                        exit_info['google_maps_link'] = coord_data['google_maps_link']
                        found_match = True
                        break
        
        # Only add if we have meaningful exit info (skip empty rows)
        if exit_info.get('exit_name') and exit_info['exit_name']:
            exits.append(exit_info)
    
    # Write to CSV
    if exits:
        fieldnames = ['exit_name', 'exit_description', 'exit_location', 'iexit_detail_link', 'latitude', 'longitude', 'google_maps_link', 'direction']
        
        with open(output_csv_path, 'w', newline='', encoding='utf-8') as csvfile:
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(exits)
        
        print(f"Successfully extracted {len(exits)} exits to {output_csv_path}")
        
        # Display first few rows as preview
        print("\nPreview of extracted data:")
        print("-" * 80)
        for i, exit in enumerate(exits[:5]):  # Show first 5 exits
            print(f"\nExit {i+1}:")
            print(f"  Name: {exit['exit_name']}")
            print(f"  Description: {exit['exit_description']}")
            print(f"  Location: {exit['exit_location']}")
            print(f"  Direction: {exit['direction']}")
            print(f"  iExit Detail Link: {exit['iexit_detail_link']}")
            print(f"  Coordinates: {exit['latitude']}, {exit['longitude']}")
            print(f"  Google Maps: {exit['google_maps_link']}")
        
        if len(exits) > 5:
            print(f"\n... and {len(exits) - 5} more exits")
            
    else:
        print("No exit data found in the HTML file")
    
    return exits

def extract_coordinates_from_javascript(html_content):
    """
    Extract coordinates and Google Maps links from JavaScript map initialization code
    """
    coordinates_data = {}
    
    # Look for the pattern: title = 'Exit X'; content = "Google Maps link"; add_marker(lat, lng)
    # Split content into lines for easier processing
    lines = html_content.split('\n')
    
    current_title = None
    current_content = None
    
    for i, line in enumerate(lines):
        line = line.strip()
        
        # Look for title assignment
        title_match = re.search(r"title\s*=\s*['\"]([^'\"]+)['\"]", line)
        if title_match:
            current_title = title_match.group(1)
        
        # Look for content with Google Maps link
        if 'maps.google.com' in line and 'content' in line:
            # Extract the Google Maps URL from the content
            maps_match = re.search(r"http://maps\.google\.com/maps\?t=m&(?:amp;)?q=loc:([+-]?\d+\.\d+)\+([+-]?\d+\.\d+)", line)
            if maps_match and current_title:
                lat = maps_match.group(1)
                lng = maps_match.group(2)
                maps_url = f"http://maps.google.com/maps?t=m&q=loc:{lat}+{lng}"
                
                coordinates_data[current_title] = {
                    'latitude': lat,
                    'longitude': lng,
                    'google_maps_link': maps_url
                }
    
    return coordinates_data

def extract_direction_from_html(soup, source_info):
    """
    Extract direction information from the HTML
    
    Args:
        soup: BeautifulSoup object of the HTML
        source_info: Information about the source (file path or URL)
        
    Returns:
        str: Direction (e.g., "Eastbound", "Westbound", "Northbound", "Southbound")
    """
    
    # Extract direction from the button element
    direction_button = soup.find('a', class_='btn btn-default btn-sm')
    if direction_button:
        direction_text = direction_button.get_text().strip()
        return direction_text
    
    # Try alternative selectors
    direction_selectors = [
        'button:contains("bound")',
        'a:contains("bound")',
        '.direction',
        '[class*="direction"]'
    ]
    
    for selector in direction_selectors:
        try:
            element = soup.select_one(selector)
            if element:
                text = element.get_text().strip()
                if any(direction in text.lower() for direction in ['east', 'west', 'north', 'south']):
                    return text
        except:
            continue
    
    # Try to extract from URL or source info
    if isinstance(source_info, str):
        source_lower = source_info.lower()
        if 'eastbound' in source_lower or 'east' in source_lower:
            return 'Eastbound'
        elif 'westbound' in source_lower or 'west' in source_lower:
            return 'Westbound'
        elif 'northbound' in source_lower or 'north' in source_lower:
            return 'Northbound'
        elif 'southbound' in source_lower or 'south' in source_lower:
            return 'Southbound'
    
    # Default fallback
    return 'Unknown'

def extract_exit_info_to_csv(html_file_path, output_csv_path):
    """
    Legacy function - Extract exit information from iExit HTML file and save to CSV
    
    Args:
        html_file_path (str): Path to the HTML file
        output_csv_path (str): Path for the output CSV file
    """
    
    # Read the HTML file
    with open(html_file_path, 'r', encoding='utf-8') as file:
        html_content = file.read()
        soup = BeautifulSoup(html_content, 'html.parser')
    
    return extract_exit_info_from_soup(soup, html_content, output_csv_path)

# Additional utility functions

def validate_iexit_url(url):
    """
    Validate if the URL is a valid iExit URL
    
    Args:
        url (str): URL to validate
        
    Returns:
        bool: True if valid, False otherwise
    """
    
    if not url:
        return False
    
    # Check if it's an iExit URL
    if 'iexitapp.com' not in url:
        return False
    
    # Check if it's an exits page
    if '/exits/' not in url:
        return False
    
    return True

def get_interstate_info_from_url(url):
    """
    Extract interstate and direction information from URL
    
    Args:
        url (str): iExit URL
        
    Returns:
        dict: Information about the interstate
    """
    
    info = {
        'interstate': 'Unknown',
        'state': 'Unknown',
        'direction': 'Unknown'
    }
    
    if not url:
        return info
    
    # Parse URL parts
    url_parts = url.split('/')
    
    try:
        # Look for interstate information
        if 'interstate' in url_parts:
            interstate_index = url_parts.index('interstate')
            if interstate_index + 1 < len(url_parts):
                info['interstate'] = url_parts[interstate_index + 1].upper()
        
        # Look for state information
        if len(url_parts) > 2:
            for i, part in enumerate(url_parts):
                if part in ['alabama', 'florida', 'georgia', 'texas', 'california', 'arizona', 'new-mexico', 'louisiana', 'mississippi']:
                    info['state'] = part.title().replace('-', ' ')
                    break
        
        # Look for direction
        if 'eastbound' in url:
            info['direction'] = 'Eastbound'
        elif 'westbound' in url:
            info['direction'] = 'Westbound'
        elif 'northbound' in url:
            info['direction'] = 'Northbound'
        elif 'southbound' in url:
            info['direction'] = 'Southbound'
    
    except (IndexError, ValueError):
        pass
    
    return info

def create_backup_csv(output_csv_path, exits):
    """
    Create a backup CSV file with timestamp
    
    Args:
        output_csv_path (str): Original CSV path
        exits (list): List of exit data
    """
    
    if not exits:
        return
    
    timestamp = time.strftime("%Y%m%d_%H%M%S")
    backup_path = output_csv_path.replace('.csv', f'_backup_{timestamp}.csv')
    
    fieldnames = ['exit_name', 'exit_description', 'exit_location', 'iexit_detail_link', 'latitude', 'longitude', 'google_maps_link', 'direction']
    
    with open(backup_path, 'w', newline='', encoding='utf-8') as csvfile:
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(exits)
    
    print(f"üíæ Backup created: {backup_path}")

def setup_logging():
    """
    Setup basic logging for the scraper
    """
    
    import logging
    
    log_file = os.path.join(os.path.dirname(__file__), 'iexit_scraper.log')
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file),
            logging.StreamHandler()
        ]
    )
    
    return logging.getLogger(__name__)

# Update the main function to use these utilities
def main():
    """Main function to run the extraction using JavaScript scraper"""
    
    # Setup logging
    logger = setup_logging()
    logger.info("Starting iExit Exit Information Extractor (JavaScript Scraper)")
    
    print("üéØ iExit Exit Information Extractor (JavaScript Scraper)")
    print("=" * 60)
    
    try:
        # Get URL from user
        url = input("Enter iExit URL: ").strip()
        if not url:
            url = "https://www.iexitapp.com/exits/interstate/i-10/alabama/eastbound"
        
        # Validate URL
        if not validate_iexit_url(url):
            print("‚ö†Ô∏è  Warning: URL doesn't appear to be a valid iExit exits page")
            confirm = input("Continue anyway? (y/n): ").strip().lower()
            if confirm != 'y':
                print("‚ùå Extraction cancelled")
                return
        
        # Get interstate info from URL
        interstate_info = get_interstate_info_from_url(url)
        print(f"üìç Detected: {interstate_info['interstate']} in {interstate_info['state']} ({interstate_info['direction']})")
        
        # Get output CSV path
        output_csv = input("Enter output CSV path (press Enter for default): ").strip()
        if not output_csv:
            safe_name = f"exits_js_{interstate_info['interstate']}_{interstate_info['state']}_{interstate_info['direction']}".replace(' ', '_').replace('-', '_').lower()
            output_csv = os.path.join(os.path.dirname(__file__), f"{safe_name}.csv")
        
        print(f"\nüöÄ Starting JavaScript scraper extraction from: {url}")
        print("üîß Note: This will open a browser window and may require manual CAPTCHA solving")
        print("üîß You will be prompted to provide cURL headers for authentication")
        logger.info(f"JavaScript scraper extraction from: {url}")
        exits = extract_exit_info_with_js_scraper(url, output_csv)
        
        # Results summary
        print(f"\n‚úÖ Extraction complete! CSV file saved at: {output_csv}")
        logger.info(f"Extraction complete. CSV saved at: {output_csv}")
        
        if exits:
            print(f"üìä Total exits extracted: {len(exits)}")
            logger.info(f"Total exits extracted: {len(exits)}")
            
            # Create backup
            create_backup_csv(output_csv, exits)
            
            # Show sample of extracted data
            print("\nüîç Sample of extracted exits:")
            for i, exit in enumerate(exits[:3]):  # Show first 3
                print(f"  {i+1}. {exit.get('exit_name', 'N/A')} - {exit.get('exit_description', 'N/A')}")
            
            if len(exits) > 3:
                print(f"  ... and {len(exits) - 3} more exits")
                
        else:
            print("‚ö†Ô∏è  No exits were extracted. Check the URL or try a different method.")
            logger.warning("No exits were extracted")
            
            # Provide troubleshooting suggestions
            print("\nüîß Troubleshooting suggestions:")
            print("  1. Verify the URL is correct and accessible")
            print("  2. Check if the website is experiencing issues")
            print("  3. Ensure you have proper internet connectivity")
            print("  4. Make sure Node.js is installed and accessible")
    
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è  Extraction interrupted by user")
        logger.info("Extraction interrupted by user")
    except Exception as e:
        print(f"\n‚ùå Extraction failed: {e}")
        logger.error(f"Extraction failed: {e}")
        
        # Provide error-specific guidance
        if "javascript" in str(e).lower() or "node" in str(e).lower():
            print("üîß JavaScript scraper error - ensure Node.js is installed")
        elif "timeout" in str(e).lower():
            print("üîß Timeout error - website may be slow, try again later")
        elif "captcha" in str(e).lower():
            print("üîß CAPTCHA encountered - manual intervention may be required")
        else:
            print("üîß For help, check the log file: iexit_scraper.log")

if __name__ == "__main__":
    main()
