# iExit Batch Processing System

This system allows you to process all the Exit_Link entries from the states.csv file in an automated batch processing mode, with built-in session management and progress tracking.

## Features

✅ **Batch Processing**: Processes 10 entries at a time (configurable)
✅ **Progress Tracking**: Saves progress and can resume from where it left off
✅ **Session Management**: Handles session expiration and requests new curl commands
✅ **Combined Output**: All results saved to a single CSV file
✅ **Error Handling**: Continues processing even if individual entries fail
✅ **Data Analysis**: Built-in tools to analyze the collected data

## Files Overview

- `batch_processor.js` - Main batch processing engine
- `batch_config.js` - Configuration settings
- `run_batch_processor.js` - Start fresh batch processing
- `resume_batch_processor.js` - Resume interrupted processing
- `analyze_batch_data.js` - Analyze collected data
- `3_coordinate_scraper.js` - Core scraping functionality (updated for batch use)

## Quick Start

### 1. Start Fresh Processing
```bash
node run_batch_processor.js
```

### 2. Resume Interrupted Processing
```bash
node resume_batch_processor.js
```

### 3. Analyze Results
```bash
node analyze_batch_data.js
```

## Configuration

Edit `batch_config.js` to customize:
- Batch size (default: 10 entries per batch)
- Delays between requests and batches
- Session timeout settings
- Output directory location

## Session Management

The system will automatically detect when your session expires (typically after 30 minutes of inactivity) and will:
1. Pause processing
2. Ask you to provide a new curl command
3. Continue processing with the fresh session

### Getting a New Curl Command

When prompted for a new curl command:
1. Open your browser
2. Go to the iExit website
3. Open Developer Tools (F12)
4. Go to Network tab
5. Navigate to any exit page
6. Right-click on the request → Copy → Copy as cURL
7. Paste the command when prompted

## Output Files

All output files are saved in the `batch_output` directory:

- `combined_exit_data.csv` - All scraped data in one file
- `batch_progress.json` - Progress tracking
- `batch_summary.json` - Final processing summary
- `data_analysis.json` - Detailed analysis results
- `cleaned_exit_data.csv` - Only successful records (generated by analyzer)

## CSV Output Format

The combined CSV includes these columns:
- `batch_id` - Which batch this record was processed in
- `processing_timestamp` - When it was processed
- `state` - State name
- `highway` - Highway name
- `source_url` - Original iExit URL
- `exit_id` - Exit identifier
- `title` - Exit title
- `exit_name` - Exit name/number
- `exit_description` - Exit description
- `exit_location` - Exit location
- `iexit_detail_link` - Link to detailed exit page
- `latitude` - GPS latitude
- `longitude` - GPS longitude
- `google_maps_link` - Google Maps link
- `direction` - Direction (North/South/East/West)
- `processing_status` - SUCCESS/ERROR/NO_DATA
- `error_message` - Error details (if any)

## Monitoring Progress

The system provides real-time progress updates:
- Current batch being processed
- Individual entry processing status
- Success/failure counts
- Estimated time remaining

## Error Handling

The system handles various error conditions:
- Network timeouts
- Cloudflare challenges
- Missing data
- Session expiration
- Browser crashes

Failed entries are tracked and can be retried later.

## Best Practices

1. **Run during off-peak hours** to minimize detection
2. **Monitor the first few batches** to ensure everything works correctly
3. **Keep curl commands ready** for session refresh
4. **Don't interrupt** during critical operations
5. **Use the analyzer** to check data quality

## Troubleshooting

### Common Issues

**Browser won't start:**
- Check if all npm packages are installed
- Ensure no other instances are running

**Session expires quickly:**
- The website may have detected automation
- Try using a different browser profile
- Adjust delays in config

**No data extracted:**
- Check if the website structure changed
- Verify the curl command is fresh
- Look at individual error messages

**Processing stops:**
- Check the progress file for last successful position
- Use the resume script to continue
- Review error logs for patterns

### Recovery Steps

1. **If processing stops unexpectedly:**
   ```bash
   node resume_batch_processor.js
   ```

2. **If you need to restart from a specific point:**
   - Edit `batch_progress.json`
   - Adjust `processedCount` and `currentBatch`
   - Run resume script

3. **If you need to analyze partial results:**
   ```bash
   node analyze_batch_data.js
   ```

## Performance Tips

- **Batch Size**: Smaller batches = more reliable, larger batches = faster
- **Delays**: Longer delays = less likely to be detected
- **Session Management**: Refresh sessions proactively
- **Error Threshold**: Set appropriate limits for consecutive errors

## Data Quality

The analyzer provides metrics on:
- Overall success rate
- State-by-state breakdown
- Coordinate availability
- Direction coverage
- Error patterns

Use these metrics to identify:
- States with low success rates
- Common error patterns
- Missing coordinate data
- Incomplete direction coverage

## Next Steps

After batch processing completes:
1. Run the analyzer to assess data quality
2. Generate the cleaned CSV for successful records
3. Import the data into your analysis pipeline
4. Identify any gaps that need manual review

## Support

If you encounter issues:
1. Check the progress and error logs
2. Review the troubleshooting section
3. Use the resume functionality
4. Analyze partial results if needed
